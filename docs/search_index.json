[["index.html", "Statistical Computing Chapter 1 Reviews 1.1 Data Types and Structures 1.2 Data Importing and Exporting 1.3 Data Cleaning 1.4 Data Visualization 1.5 Hands-on Exercises", " Statistical Computing YOUR NAME HERE 2024-10-31 Chapter 1 Reviews 1.1 Data Types and Structures 1.1.1 Data Types There are different kinds of values in R that can be manipulated in variables. They most commonly used are; strings, numerics(integers and floats) and boolean values. The function class can be used to find the data type. Try it! Before diving deep into data types, lets create a value with a random value for instance age and finds its data type. age &lt;- 27 class(age) ## [1] &quot;numeric&quot; The age is a \"numeric\" data type variable, interesting? Lets explore different data types and their examples; Integers: These are whole numbers without dev=cimal point(e.g., 10, -5). In R, it is specified with the L suffix like 10L. Floats: These are numbers with decimal points(e.g. 3.14, -2.718). R refers them as numerics. Boolean(Logical): True or False values, represented as TRUE or FALSE in R. They are crucial in conditional statements. Strings(Character): These are text values enclosed in quotes(e.g. \"Hello world\" , names like \"John\", \"Mustafa\", \"Patel\", variable names like \"age\", \"gender\", \"salary\") You will often deal with mixed data types when analyzing real-world data sets therefore understanding these will help you handles any data set! Examples Lets have some fun! We will create different variables and find their data types; age &lt;- 34L age &lt;- 34L class(age) ## [1] &quot;integer&quot; weight &lt;- 68.2 weight &lt;- 68.2 class(weight) ## [1] &quot;numeric&quot; name &lt;- \"Mustafa\" name &lt;- &quot;Mustafa&quot; class(name) ## [1] &quot;character&quot; is_winter &lt;- FALSE is_winter &lt;- FALSE class(is_winter) ## [1] &quot;logical&quot; You see how simple it is to find the data type of different variables in R! Remember the class function returns any number whether with decimal or whole as \"numeric\". It only returns \"integer\" when there is a suffix L. Practical Exercise Try out the practical exercise below to test your understanding in data types Find the data type of 98.03 using class() function. Assign the value 98.03 to variable height and find data type of height. There are 27 goats in a field, assign the quantity of goats to a variable goats and find the data type of the variable goats. Remember to add suffix L to the value 27. Find the data type of the value \"school\" using the class() function. Assign your first name to a variable firstname and find its data type. Remember to enclose it in quotation marks Create a variable is_student and assign it the value TRUE. Use the class() function to find its data type. Solution Find the data type of 98.03 using class() function. class(98.03) ## [1] &quot;numeric&quot; Assign the value 98.03 to variable height and find data type of height. height &lt;- 98.03 class(height) ## [1] &quot;numeric&quot; There are 27 goats in a field, assign the quantity of goats to a variable goats and find the data type of the variable goats. Remember to add suffix L to the value 27. goats &lt;- 27L class(goats) ## [1] &quot;integer&quot; Find the data type of the value \"school\" using the class() function. class(&quot;school&quot;) ## [1] &quot;character&quot; Assign your first name to a variable firstname and find its data type. Remember to enclose it in quotation marks firstname &lt;- &quot;Bryant&quot; # Any name will work class(firstname) ## [1] &quot;character&quot; Create a variable is_student and assign it the value TRUE. Use the class() function to find its data type. is_student &lt;- TRUE class(is_student) ## [1] &quot;logical&quot; ________________________________________________________________________________ 1.1.2 Data Structures This is the organization of data into or multiple data values in specific structures, they include vectors, matrix and data frames. Lets explore the mentioned data structures and their examples; Vector: This is a sequence of elements of the same data types(e.g., `c(1, 2, 3) is a numeric vector) Matrix: This is a two-dimensional data structure with rows and columns, where all elements are of the same type(e.g. numbers). Data Frames: This is the most common R data structure for handling tabular data(like an excel sheet). A data frame can contain different data types in each column unlike matrices and vectors. Data frames are central to real-world data analysis. You will work with them to analyze, transform, and visualize data sets, whether you are calculating averages or identifying trends. The is.vector, is.matrix and is.data.frame functions are used to confirm if the variable in question is a vector, matrix or data frame respectively. Examples Lets have some fun! We will create different data structures and find their data types: Create a vector, marks to store the values, 23, 67, 98, 34, 98, 21. Print the vector to the console and use is.vector function to confirm if its a actually a vector. marks = c(23, 67, 98, 34, 98, 21) print(marks) # print to the console ## [1] 23 67 98 34 98 21 is.vector(marks) # find its data structure ## [1] TRUE Create a matrix with values from 1 to 9 and use the is.matrix function to find to confirm if its really a matrix. vector1 = seq(1, 9) # Convert to matrix ## create by column m1=matrix(vector1, ncol=3) print(m1) # print the matrix to the console ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 is.matrix(m1) # confirms if its really a matrix ## [1] TRUE Create a data.frame from the above matrix. Add the column names as \"A\", \"B\", \"C\". Confirm if its really a matrix. var_names &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # vector to store variable names m1_df &lt;- data.frame(var_names, m1) # create the data frame print(m1_df) #print to the console ## var_names X1 X2 X3 ## 1 A 1 4 7 ## 2 B 2 5 8 ## 3 C 3 6 9 is.data.frame(m1_df) #confirms if its really a data.frame. ## [1] TRUE Practical Exercise Try out the exercise below to test your understanding in R data structures; Create a vector named height with the values 120.1, 118, 123.4, 130.8, 115.2. Use the is.vector to confirm that the created variable is a vector. Use length() function to count the number of elements in the vector. Create a matrix m1 from the vector v1 where v1 &lt;- seq(1, 12) with three columns. Use the is.matrix function to confirm if the said variable is a matrix. Access the third column by running the command m1[, 3]. Access the second row by running the command m1[2,]. Create a data frame students_df with the columns \"Name\", \"Age\", and \"Marks\" for three students. Where Name &lt;- c(\"Pragya\", \"Thomas\", \"Ali\"), Age &lt;- c(21, 19, 23) and Marks &lt;- c(68, 72, 67). Solution Create a vector named height with the values 120.1, 118, 123.4, 130.8, 115.2. Use the is.vector to confirm that the created variable is a vector. height &lt;- c(120.1, 118, 123.4, 130.8, 115.2) is.vector(height) ## [1] TRUE Use length() function to count the number of elements in the vector. length(height) # count the number of elements ## [1] 5 Create a matrix m1 from the vector v1 where v1 &lt;- seq(1, 12) with three columns. Use the is.matrix function to confirm if the said variable is a matrix. v1 &lt;- seq(1, 12) # Create vector v1 m1 &lt;- matrix(v1, ncol=3) # create a matrix from the vector m1 ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 is.matrix(m1) # Confirm if its a matrix ## [1] TRUE Access the third column by running the command m1[, 3]. m1[, 3] # Access the third columns ## [1] 9 10 11 12 Access the second row by running the command m1[2,]. m1[2,] # Access of the second row ## [1] 2 6 10 Create a data frame students_df with the columns \"Name\", \"Age\", and \"Marks\" for three students. Where Name &lt;- c(\"Pragya\", \"Thomas\", \"Ali\"), Age &lt;- c(21, 19, 23) and Marks &lt;- c(68, 72, 67). student_df &lt;- data.frame( &quot;Name&quot;= c(&quot;Pragya&quot;, &quot;Thomas&quot;, &quot;Ali&quot;), &quot;Age&quot;=c(21, 19, 23), &quot;Marks&quot;=c(68, 72, 67) ) student_df ## Name Age Marks ## 1 Pragya 21 68 ## 2 Thomas 19 72 ## 3 Ali 23 67 ________________________________________________________________________________ 1.2 Data Importing and Exporting Importing and exporting data is the foundation of data analysis workflows. The main two types of data files used are CSV and excel files. CSV Files: R can easily import CSV files using read.csv(\"filename.csv\"). The CSV is one of the most common formats you will encounter. Excel Files: For excel files, you can use the readxl package with the function read_excel. Try it: Let’s have some fun by importing; From CSV file m1_imported &lt;- read.csv(&quot;data/m1.csv&quot;) # import the csv data set m1_imported # Display the data ## X V1 V2 V3 ## 1 1 1 5 9 ## 2 2 2 6 10 ## 3 3 3 7 11 ## 4 4 4 8 12 From Excel file library(readxl) students_imported &lt;- read_excel(&quot;data/students.xlsx&quot;) # Import the data students_imported # Display the data ## # A tibble: 3 × 3 ## Name Age Marks ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Pragya 21 68 ## 2 Thomas 19 72 ## 3 Ali 23 67 After data wrangling, manipulation and processing, the end product(processed data) can be saved for further use. The data can also be shared to others. Lets explore how export the CSV and Excel files To CSV: You can save your data to CSV format using write.csv(data, \"filepath.csv\"). To Excel To write to Excel, you can use write.xlsx(data, \"filepath.xlsx\") from the openxlsx package. Try it: Lets export the previously imported data set locally To CSV write.csv(m1_imported, &quot;data/m1_exported.csv&quot;) # Write the data set locally Excel File library(openxlsx) write.xlsx(students_imported, &quot;data/students_exported.xlsx&quot;) Practical Exercise 1.3 Data Cleaning Before you analyze data, it is crucial to ensure that it is clean. Here are some common issues in data cleaning; Null Values: Missing data can distort your analysis. Functions like is.na() and na.omit() are used to detect and remove null values respectively. Null values can also be imputed by filling the missing values with the most relevant value for instance mean, mode or median of the variable, zero, or any dedicated value. Duplicated Records: Duplicates can cause bias in results and they can detected using the duplicated() function. This duplicated records can be removed by unique() function from R or distinct() from dplyr package. Outliers: These are extreme values that don’t follow the general trend. The use of summary statistics(specifically IQR) and boxplots can be used to cap these values based on the context. Data cleaning is like polishing a diamond-it ensures the data is ready for analysis, free from distortions like missing values or outliers that can skew your insights. Try it: Lets have some fan! We will create a random data set, identify all data issues and address them by cleaning. Create a sample data set # Create a dataset set.seed(42) df &lt;- data.frame( Product = c(&#39;Shoes&#39;, &#39;Laptop&#39;, &#39;Watch&#39;, &#39;Phone&#39;, &#39;Shoes&#39;, &#39;Watch&#39;, &#39;Laptop&#39;, &#39;Shoes&#39;, &#39;Laptop&#39;, &#39;Phone&#39;), Sales = c(150, 500, NA, 300, 150, 1000, 500, 150, 500, 300), # Outlier in Sales (1000) Category = c(&#39;Fashion&#39;, &#39;Tech&#39;, &#39;Fashion&#39;, &#39;Tech&#39;, &#39;Fashion&#39;, &#39;Fashion&#39;, &#39;Tech&#39;, &#39;Fashion&#39;, &#39;Tech&#39;, &#39;Tech&#39;), Discount = c(10, 0, 20, 5, 10, 20, 0, 10, 0, 5), Returns = c(2, 0, 1, 0, 2, 1, 0, 2, 0, 0), Profit = c(30, 100, NA, 70, 30, 500, 100, 30, 100, 70) # Outlier in Profit (500) ) # Add duplicated rows df &lt;- rbind(df, df[2:3, ]) # View the dataset head(df) ## Product Sales Category Discount Returns Profit ## 1 Shoes 150 Fashion 10 2 30 ## 2 Laptop 500 Tech 0 0 100 ## 3 Watch NA Fashion 20 1 NA ## 4 Phone 300 Tech 5 0 70 ## 5 Shoes 150 Fashion 10 2 30 ## 6 Watch 1000 Fashion 20 1 500 Count the null values sum(is.na(df)) ## [1] 4 There are 4 null values in the data set. Let’s handle the null values by filling them with mean of the respective variables. # Fill missing Sales and Profit with the mean of the respective columns df$Sales[is.na(df$Sales)] &lt;- mean(df$Sales, na.rm = TRUE) df$Profit[is.na(df$Profit)] &lt;- mean(df$Profit, na.rm = TRUE) # View the data set after handling null values head(df) ## Product Sales Category Discount Returns Profit ## 1 Shoes 150 Fashion 10 2 30 ## 2 Laptop 500 Tech 0 0 100 ## 3 Watch 405 Fashion 20 1 113 ## 4 Phone 300 Tech 5 0 70 ## 5 Shoes 150 Fashion 10 2 30 ## 6 Watch 1000 Fashion 20 1 500 # Count the null values in the data set to confirm the operation sum(is.na(df)) ## [1] 0 The null values are now filled and the data set is complete. Let’s find if there exists some duplicated records and how many are they? # Count the duplicated rows sum(duplicated(df)) ## [1] 7 # Shape of the data set dim(df) ## [1] 12 6 There are 7 duplicated rows. We will remove the duplicated records and retain only one row of the same kind. This can be achieved using unique() from base R or distinct() from dplyr package. In this case we will be using the distinct command. # Load the required libraries library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Remove duplicated rows df_cleaned &lt;- df %&gt;% distinct() # Count the duplicated records sum(duplicated(df_cleaned)) ## [1] 0 # Shape of the data set dim(df_cleaned) ## [1] 5 6 The data has no duplicated records and it is evident that 7 records(duplicated) were deleted. They are only 5 rows remaining. The last step of data cleaning in this case is to identify outliers in the Sales and Profit, and remove them using the IQR method. # Use the IQR method to detect outliers in Sales and Profit Q1_sales &lt;- quantile(df_cleaned$Sales, 0.25) Q3_sales &lt;- quantile(df_cleaned$Sales, 0.75) IQR_sales &lt;- Q3_sales - Q1_sales Q1_profit &lt;- quantile(df_cleaned$Profit, 0.25) Q3_profit &lt;- quantile(df_cleaned$Profit, 0.75) IQR_profit &lt;- Q3_profit - Q1_profit # Filter out outliers df_cleaned &lt;- df_cleaned %&gt;% filter(!(Sales &lt; (Q1_sales - 1.5 * IQR_sales) | Sales &gt; (Q3_sales + 1.5 * IQR_sales))) %&gt;% filter(!(Profit &lt; (Q1_profit - 1.5 * IQR_profit) | Profit &gt; (Q3_profit + 1.5 * IQR_profit))) # Find out how many records were affected dim(df_cleaned) ## [1] 4 6 Only one row had outliers and was removed. The data is now clean and ready for further analysis. Practical Exercise Solution ________________________________________________________________________________ 1.4 Data Visualization **____What is Data Visualization____** ggplot is one of the most popular and flexble data visualization libraries in R. It follows the grammar of graphics philosophy, allowing you o build plots in layers. Here are some of the basic plots in data visualization; Scatter Plots: used to visualize the relationship between two variables in R. Bar Charts: used to compare categorical data. Histograms: used to represent distribution of a single continuous variable. Visualizations are powerful tools that help you see patterns and insights that raw data might hide. A well-made plot can communicate your findings more effectively than numbers alone. Try it: Lets use the above data set that we cleaned to plot simple charts in R using ggplot library. Install the package if not installed install.packages(&quot;ggplot2&quot;) Load the library library(ggplot2) 1.5 Hands-on Exercises Solution ________________________________________________________________________________ "],["functions.html", "Chapter 2 Functions 2.1 Writing Functions 2.2 Calling the Functions 2.3 Function Documentation 2.4 Hands-on Exercise", " Chapter 2 Functions In programming, functions are like little blocks of code that perform a specific task. Think of them as reusable instructions that you can call whenever you need them. Here’s why functions are super helpful: Avoid repetition: Instead of writing the same code multiple times, you can just call the function. Cleaner code: Your code becomes easier to read and maintain because functions help organize it better. Easier debugging: When something goes wrong, you only need to check the function itself rather than searching through your entire program. Why Use Functions? Imagine having to rewrite a set of instructions every time you need them! With functions, you write the code once and reuse it as many times as you want. A good rule of thumb is: if you expect to run a specific set of instructions more than twice, create a function for it. What Can Functions Do? Functions are flexible and can be used for many different purposes: Take input (called arguments) Process the input based on what the function is meant to do Return a result after completing the task 2.1 Writing Functions Lets take a tour on different types of functions in R before diving deep into writing functions. This will help you understand when to write functions and when to use readily-available functions. There are three main types of functions: User-Defined Functions (UDF) – Custom functions you write for your specific needs. Built-in functions – These come pre-loaded in R. Example: mean() Package functions – Functions from external R packages you can install. Example: ggplot() and select() from ggplot2 and dplyr respectively. 2.1.1 User-Defined Functions The best way to grasp how functions work in R is by creating your own! These are called* User-Defined Functions (UDFs), and they allow you to design custom tasks that fit your needs. In R, functions typically follow this format: function_name &lt;- function(argument_1, argument_2) { # Function body (your instructions go here) return(output) } Let’s break down the key elements: Function Name: This is how you’ll call your function later. When you create a function, you assign it a name and save it as a new object. For example, if you name your function calculate_mean, that’s the name you’ll use every time you want to run the function. Arguments (also called Parameters): Arguments are placed inside the parentheses. They tell the function what input to expect or how to modify its behavior. Think of them as placeholders for the data you’ll provide later when you run the function. Function Body: Inside the curly brackets {}, you’ll write the instructions that the function will follow to accomplish the task. This is the “heart” of the function. Return Statement: The return() function tells R what result to give you after the function finishes its job. It’s optional, but it helps if you want to store the function’s result in a variable. Let’s write a simple function that calculates the mean (average) of two numbers: mean_two_numbers &lt;- function(num_1, num_2) { mean &lt;- (num_1 + num_2) / 2 return(mean) } How to Use the Function: To find the mean of 10 and 20, simply call the function like this: mean_two_numbers(10, 20) ## [1] 15 Let’s add a few more simple tasks: writing a function that calculates the difference between two numbers. Why is this important? Well, imagine you have two values and you want to find their difference—that’s exactly what this function will help us do! # Function to calculate the difference between two numbers calculate_difference &lt;- function(x, y) { # Subtract the second number (y) from the first number (x) difference &lt;- x - y # Return the difference result so we can use it later return(difference) } You see!: x and y are our arguments: These are the two numbers we’ll use in our calculation. The subtraction happens inside the function: We simply subtract y from x and store the result in difference. Finally, we return the difference: This way, we can use the result when we call the function. Now, let’s put it to the test! We’ll run the function with different sets of numbers and see what we get: calculate_difference(10, 5) # 10 - 5 = 5 ## [1] 5 calculate_difference(25, 15) # 25 - 15 = 10 ## [1] 10 calculate_difference(50, 30) # 50 - 30 = 20 ## [1] 20 Notice how easy it is to calculate the difference between any two numbers by just calling our function? That’s the power of writing your own functions—they make life a lot easier! Now, lets make it more interesting! How about a function that greets you by name? We can do the same in R by creating a simple function that takes someone’s name and returns a greeting. Here is how we do it: # Function to greet a student by their name greet_student &lt;- function(student_name) { # Create a personalized greeting greeting &lt;- paste(&quot;Hello&quot;, student_name, &quot;!&quot;) # Return the greeting so we can use it later return(greeting) } Remember! We use student_name as the argument: This is where you pass in the name of the student. We combine \"Hello\" with the name: The paste() function(that is an -in-built function which will discuss later in the course) helps us put the pieces together to form a full sentence. Return the greeting: The function gives us back a customized message, ready to greet anyone! Lets try it out with different names greet_student(&quot;John&quot;) # Hello John! ## [1] &quot;Hello John !&quot; greet_student(&quot;Alice&quot;) # Hello Alice! ## [1] &quot;Hello Alice !&quot; greet_student(&quot;Michael&quot;) # Hello Michael! ## [1] &quot;Hello Michael !&quot; Remember to try it out with your name! Key Takeaways: By writing these two simple functions, you’ve already tackled a lot of important concepts in R! You now know: How to create a function. How to pass arguments (inputs/parameters) to a function. How to return a result that you can use later. Practical Exercise In this exercise, you’ll get hands-on practice creating your own functions in R. Follow the instructions below to write functions that perform specific tasks. Remember to test your functions with different input values! Create a function called add_numbers that takes two arguments, a and b, and returns their sum. Write a function named is_even that takes a single argument, num, and returns \"Even\" if the number is even, or \"Odd\" if it’s odd. Create a function called find_max that takes three arguments and returns the largest of the three numbers. Solution Create a function called add_numbers that takes two arguments, a and b, and returns their sum. # Function to calculate the sum of two numbers sum_two_numbers &lt;- function(x, y) { sum &lt;- x + y return(sum) } # Test the function with different values sum_two_numbers(5, 10) # Output: 15 ## [1] 15 sum_two_numbers(20, 30) # Output: 50 ## [1] 50 sum_two_numbers(100, 200) # Output: 300 ## [1] 300 Write a function named is_even that takes a single argument, num, and returns \"Even\" if the number is even, or \"Odd\" if it’s odd. # Function to check if a number is even or odd check_even_odd &lt;- function(number) { if (number %% 2 == 0) { return(&quot;Even&quot;) } else { return(&quot;Odd&quot;) } } # Test the function with different numbers check_even_odd(4) # Output: &quot;Even&quot; ## [1] &quot;Even&quot; check_even_odd(7) # Output: &quot;Odd&quot; ## [1] &quot;Odd&quot; check_even_odd(10) # Output: &quot;Even&quot; ## [1] &quot;Even&quot; Create a function called find_max that takes three arguments and returns the largest of the three numbers. # Function to find the maximum of three numbers max_of_three &lt;- function(a, b, c) { max_value &lt;- max(a, b, c) # Use the built-in max function return(max_value) # Return the maximum value } # Test the function with different values max_of_three(10, 20, 5) # Output: 20 ## [1] 20 max_of_three(3, 1, 2) # Output: 3 ## [1] 3 max_of_three(7, 15, 12) # Output: 15 ## [1] 15 ________________________________________________________________________________ 2.1.2 Built-in Fuctions We have learned how to create our own user-defined functions (UDFs) to perform specific tasks. Now, let’s dive deeper into R’s capabilities by exploring its built-in functions. These handy tools are readily available for you to use anytime, making your coding experience even smoother. R is packed with a treasure trove of built-in functions that allow you to perform a variety of tasks with just a few simple commands. Whether you’re crunching numbers or analyzing data, these functions are your best friends. Here’s a sneak peek at some of the most useful built-in functions in R: print(): This function displays an R object right on your console. It’s like saying, “Hey, look at this!” print(&quot;Hello Mum&quot;) ## [1] &quot;Hello Mum&quot; min() and max(): Need to find the smallest or largest number in a bunch? These functions will do just that for a numeric vector. sum(): Want to add up a series of numbers? Use sum() to get the total of a numeric vector. mean(): This function calculates the average of your numbers. Perfect for when you need to find the middle ground! range(): Curious about the minimum and maximum values of your numeric vector? range() has you covered. str(): Want to understand the structure of an R object? str() will give you a clear picture of what’s inside. ncol(): If you’re working with matrices or data frames, this function tells you how many columns you have. length(): This one returns the number of items in an R object, whether it’s a vector, a list, or a matrix. Here’s a quick example to show you how easy it is to use these functions with a vector of numbers: v &lt;- c(1, 3, 0.2, 1.5, 1.7) # Create a vector print(v) # Display the vector ## [1] 1.0 3.0 0.2 1.5 1.7 sum(v) # Calculate the total sum ## [1] 7.4 mean(v) # Find the average ## [1] 1.48 length(v) # Get the number of elements ## [1] 5 As you can see, working with R’s built-in functions is straightforward and super helpful. Start experimenting with these functions and watch how they can simplify your coding experience! Key Takeaways: By completing this exercise, you’ve already tackled several important concepts in R! You now know: How to create a vector and use it for calculations. How to utilize built-in functions like sum(), max(), min(), mean(), and length(). How to derive meaningful statistics from data using R’s built-in capabilities R has a wealth of resources on this topic, and as you gain more experience and knowledge, you’ll uncover even more advanced built-in functions that can simplify your programming tasks. Practical Exercise In this exercise, you are required to create a vector named numbers that contains the following values: 4, 8, 15, 16, 23, 42. After creating the vector, you will use various built-in functions to analyze it based on the instructions below; Use the sum() function to calculate the total of the numbers vector. Use the max() function to find the maximum value in the numbers vector. Use the min() function to find the minimum value in the numbers vector. Use the mean() function to calculate the average of the numbers vector. Use the length() function to find out how many elements are in your numbers vector. Solution In this exercise, you are required to create a vector named numbers that contains the following values: 4, 8, 15, 16, 23, 42. After creating the vector, you will use various built-in functions to analyze it based on the instructions below; # Create a vector numbers &lt;- c(4, 8, 15, 16, 23, 42) Use the sum() function to calculate the total of the numbers vector. sum(numbers) ## [1] 108 Use the max() function to find the maximum value in the numbers vector. max(numbers) ## [1] 42 Use the min() function to find the minimum value in the numbers vector. min(numbers) ## [1] 4 Use the mean() function to calculate the average of the numbers vector. mean(numbers) ## [1] 18 Use the length() function to find out how many elements are in your numbers vector. length(numbers) ## [1] 6 ________________________________________________________________________________ 2.1.3 Package Functions Just like we’ve learned about User-Defined and Built-in Functions, R also provides a vast number of additional functions through packages. These packages extend R’s capabilities and allow you to perform specific tasks, from data manipulation to machine learning, with ease. What are R Package Functions? Packages in R are collections of R functions, data, and compiled code that are stored in a well-defined format. While R comes with a set of built-in functions, packages allow you to go beyond the basic functionality. You can install and load packages based on the task you want to accomplish. Think of package functions as tools in a toolbox: not everything is built-in, but by adding specific tools, you can perform new tasks easily. Lets explore how to get started using the functions; Installing and Loading Packages To use functions from a package, you first need to install the package and load it into your R session. install.packages(&quot;package_name&quot;) Every time you start a new R session if you want to use the functions from that package. Load the package by; library(package_name) To put this into real-life action, let’s learn about the dplyr package, which is commonly used for data manipulation. It contains many useful functions to work with data frames or tibbles (a modern version of data frames). Here’s an example of how to install and load dplyr, and use some of its core functions. Install the package install.packages(&quot;dplyr&quot;) # Install it once Load the package library(dplyr) # Load it whenever you need to use it Let’s explore a few package functions from dplyr: select(): Chooses specific columns from a dataset. filter(): Filters rows based on conditions. mutate(): Adds new variables (columns) or modifies existing ones. summarise(): Summarizes data, such as calculating the mean or total We will create a data frame to demonstrate how to use functions from the dplyr package. # Create a data frame for demonstration data &lt;- data.frame( Name = c(&quot;John&quot;, &quot;Jane&quot;, &quot;David&quot;, &quot;Anna&quot;), Age = c(28, 34, 22, 19), Score = c(85, 90, 88, 92) ) # 1. Select only the Name and Score columns selected_data &lt;- select(data, Name, Score) selected_data ## Name Score ## 1 John 85 ## 2 Jane 90 ## 3 David 88 ## 4 Anna 92 # 2. Filter rows where Score is greater than 88 filtered_data &lt;- filter(data, Score &gt; 88) filtered_data ## Name Age Score ## 1 Jane 34 90 ## 2 Anna 19 92 # 3. Add a new column that increases Score by 10 mutated_data &lt;- mutate(data, New_Score = Score + 10) mutated_data ## Name Age Score New_Score ## 1 John 28 85 95 ## 2 Jane 34 90 100 ## 3 David 22 88 98 ## 4 Anna 19 92 102 # 4. Calculate the average age summary_data &lt;- summarise(data, Average_Age = mean(Age)) summary_data ## Average_Age ## 1 25.75 In this example, we used functions from the dplyr package to select columns, filter rows, modify data, and summarize it! Key Takeaways: By learning about R package functions, you’ve unlocked even more tools to work efficiently in R. Here’s what you’ve learned today: How to install and load R packages. How to use package functions like those in dplyr for data manipulation. How to perform tasks like selecting columns, filtering data, and summarizing values. Packages in R allow you to extend the functionality of the base language for specific tasks. With packages, R becomes an even more powerful tool, allowing you to work with more advanced data sets and perform complex operations with ease! Practical Exercise In this exercise, you will use the functions from the dplyr package to manipulate the iris data set. Remember the dplyr package is installed by: install.packages(&quot;dplyr&quot;) and is loaded by: library(dplyr) The iris data set is loaded by data(&quot;iris&quot;) # view the first few columns head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Solve the following questions; Use the select function to select the Sepal.Length, Sepal.Width, and Species columns. Use the filter function to filter rows where Sepal.Length is greater than 5. Use the mutate function to create a new column Sepal.Ratio that divides Sepal.Length by Sepal.Width. Solution In this exercise, you will use the functions from the dplyr package to manipulate the iris data set. Remember the dplyr package is installed by: install.packages(&quot;dplyr&quot;) and is loaded by: library(dplyr) The iris data set is loaded by data(&quot;iris&quot;) # view the first few columns head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Solve the following questions; Use the select function to select the Sepal.Length, Sepal.Width, and Species columns. selected_iris &lt;- select(iris, Sepal.Length, Sepal.Width, Species) head(selected_iris) ## Sepal.Length Sepal.Width Species ## 1 5.1 3.5 setosa ## 2 4.9 3.0 setosa ## 3 4.7 3.2 setosa ## 4 4.6 3.1 setosa ## 5 5.0 3.6 setosa ## 6 5.4 3.9 setosa Use the filter function to filter rows where Sepal.Length is greater than 5. filtered_iris &lt;- filter(iris, Sepal.Length&gt;5) head(filtered_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 5.4 3.9 1.7 0.4 setosa ## 3 5.4 3.7 1.5 0.2 setosa ## 4 5.8 4.0 1.2 0.2 setosa ## 5 5.7 4.4 1.5 0.4 setosa ## 6 5.4 3.9 1.3 0.4 setosa Use the mutate function to create a new column Sepal.Ratio that divides Sepal.Length by Sepal.Width. updated_iris &lt;- mutate(iris, Sepal.Ratio=Sepal.Length/Sepal.Width) head(updated_iris$Sepal.Ratio) ## [1] 1.457143 1.633333 1.468750 1.483871 1.388889 1.384615 ________________________________________________________________________________ 2.1.4 Type of arguments in R functions Now that we’ve learned and explored different types of functions, let’s dive into function arguments to strengthen your understanding of writing functions. Arguments are essential components of any function. Although it’s possible to write a function without parameters, like the example below, most functions do require arguments to tell them what data to process hello &lt;- function() { print(&#39;Hello, my friend&#39;) } Why Arguments Matter Arguments are the input for functions. They allow us to give the function specific values to work with. If we want a function to handle different cases or data, arguments give us that flexibility. When defining arguments, you include them inside the parentheses of the function definition, separated by commas. Generally, functions with more arguments tend to be more complex, but they also offer greater control over what the function does. # Creating a function with arguments my_function &lt;- function(argument1, argument2){ # function body } Handling Missing Arguments Whenever you create a function with parameters, you must provide the values for those parameters when calling the function. Otherwise, R will return an error. For example, if you forget to supply both numbers in a function to calculate their mean, the function won’t work. But you can avoid this issue by using default arguments. These are preset values that the function will use if you don’t provide them during the call. Let’s modify our mean function to demonstrate: mean_two_numbers &lt;- function(num_1, num_2 = 30) { mean &lt;- (num_1 + num_2) / 2 return (mean) } In this version, if you only provide one value when calling the function, R will automatically use the default value for the second number (which is 30 in this case): mean_two_numbers(num_1 = 10) ## [1] 20 You now understand how arguments work and the importance of default values in making functions more flexible and error-proof. Practical Exercise Create a function greet that prints a simple message like \"Hello, welcome to R programming!\". Write a function multiply_numbers that takes two arguments, a and b, and returns the product of these numbers Create a function calculate_total that accepts two arguments, price and tax_rate. Set a default value of tax_rate = 0.15 (15%). Solution Create a function greet that prints a simple message like \"Hello, welcome to R programming!\". greet &lt;- function() { print(&quot;Hello, welcome to R programming!&quot;) } # Call the function greet() ## [1] &quot;Hello, welcome to R programming!&quot; Write a function multiply_numbers that takes two arguments, a and b, and returns the product of these numbers. multiply_numbers &lt;- function(a, b) { return(a * b) } # Call the function multiply_numbers(6, 8) ## [1] 48 Create a function calculate_total that accepts two arguments, price and tax_rate. Set a default value of tax_rate = 0.15 (15%). calculate_total &lt;- function(price, tax_rate = 0.15) { total &lt;- price + (price * tax_rate) return(total) } # Call the function calculate_total(price=160) ## [1] 184 ________________________________________________________________________________ 2.1.5 Understanding Return Values in R Functions In many programming languages, functions take data as input and produce some result as output. Often, you must use a return statement to explicitly give back the result. Otherwise, the value might only be visible inside the function and not available to use later. But in R, the situation is a little more relaxed! In R, a function will always return a value that can be stored in a variable, even without a return statement. However, for clarity and good practice, it’s still helpful to include return to show your intent. Let’s walk through an example: mean_sum &lt;- function(num_1, num_2) { mean &lt;- (num_1 + num_2) / 2 sum &lt;- num_1 + num_2 return(list(mean = mean, sum = sum)) } Now, calling the function: results &lt;- mean_sum(10, 20) print(results) # You&#39;ll see both the mean and sum printed ## $mean ## [1] 15 ## ## $sum ## [1] 30 2.2 Calling the Functions In previous sections, we’ve seen how to call functions with different arguments. Now, let’s dig a little deeper into how R works behind the scenes when you pass arguments to a function. R allows two main ways of passing arguments: By position – The arguments are passed in the same order as the function definition. By name – You explicitly mention the argument name and its value. You can also mix these two strategies! Let’s explore these options using an example. Here’s a simple function that takes two arguments: name and surname. hello &lt;- function(name, surname) { print(paste(&#39;Hello&#39;, name, surname)) } Lets call the function using different strategies; By Position You pass the arguments in the exact order the function expects. hello(&#39;Jane&#39;, &#39;McCain&#39;) ## [1] &quot;Hello Jane McCain&quot; By Name When using this method, the order doesn’t matter. You just specify the argument names. hello(surname = &#39;McCain&#39;, name = &#39;Jane&#39;) ## [1] &quot;Hello Jane McCain&quot; Mixing Position and Name You can mix both approaches. Named arguments are matched first, then the remaining ones are matched by position hello(surname = &#39;McCain&#39;, &#39;Jane&#39;) ## [1] &quot;Hello Jane McCain&quot; This flexibility can make your code easier to read and maintain, especially when functions have many arguments! 2.3 Function Documentation Finally when writing functions, it’s always a good idea to provide documentation to guide users on how to use the function. This is especially important when dealing with complex functions or when the function is shared with others. One simple way to add documentation is by including comments in the body of your function. These comments explain what each part of the function does. This is an informal method, but it helps both you and others quickly understand what’s happening in the function. Here’s an example: hello &lt;- function(name, surname) { # Say hello to a person with their name and surname print(paste(&#39;Hello,&#39;, name, surname)) } If you call the function without executing it, you’ll see its structure along with the comments: hello ## function(name, surname) { ## # Say hello to a person with their name and surname ## print(paste(&#39;Hello,&#39;, name, surname)) ## } If your function is part of a larger package and you want it to be properly documented, you should write formal documentation in a separate .Rd file. These files store structured documentation, which you can access using ?function_name in R, similar to the help file you see for built-in functions like ?mean. Formal documentation includes details such as: Function name and description. Arguments and their roles. Examples of how to use the function. Output that the function returns. This approach ensures that users can easily understand and use your function, even in complex packages. 2.4 Hands-on Exercise You will attempt this hands-on exercise to confirm your understanding of functions. For one of the functions you created, add comments inside the function to explain what each part of the function does. Create a User-Defined Function (UDF) named calculate_area that takes two arguments: length and width. The function should return the area of a rectangle. Create a vector named values with the numbers 4, 8, 15, 16, 23, 42. Use the built-in sum() function to calculate the total of the values vector and print the result. Write a function named greet that takes one argument, student_name, and prints a greeting. Modify the function to have a default argument that greets a \"Student\" if no name is provided. Create a function named mean_and_median that takes a numeric vector as an argument and returns both the mean and median of that vector as a list. Solution Create a User-Defined Function (UDF) named calculate_area that takes two arguments: length and width. The function should return the area of a rectangle. calculate_area &lt;- function(length, width) { # Calculate the area by multiplying length and width area &lt;- length * width # Return the calculated area return(area) } # Example usage of the calculate_area function area_result &lt;- calculate_area(5, 10) # Length: 5, Width: 10 print(paste(&quot;Area of rectangle:&quot;, area_result)) ## [1] &quot;Area of rectangle: 50&quot; Create a vector named values with the numbers 4, 8, 15, 16, 23, 42. Use the built-in sum() function to calculate the total of the values vector and print the result. values &lt;- c(4, 8, 15, 16, 23, 42) # Use the built-in sum() function to calculate the total of the values vector total &lt;- sum(values) # Print the result print(paste(&quot;Total of values vector:&quot;, total)) ## [1] &quot;Total of values vector: 108&quot; Write a function named greet that takes one argument, student_name, and prints a greeting. Modify the function to have a default argument that greets a \"Student\" if no name is provided. greet &lt;- function(student_name = &quot;Student&quot;) { # Print a greeting using the provided name or default to &quot;Student&quot; print(paste(&quot;Hello,&quot;, student_name)) } # Example usage of the greet function with a provided name greet(&quot;John&quot;) # Should print &quot;Hello, John&quot; ## [1] &quot;Hello, John&quot; # Example usage of the greet function without providing a name greet() ## [1] &quot;Hello, Student&quot; Create a function named mean_and_median that takes a numeric vector as an argument and returns both the mean and median of that vector as a list. mean_and_median &lt;- function(num_vector) { # Calculate the mean of the vector mean_value &lt;- mean(num_vector) # Calculate the median of the vector median_value &lt;- median(num_vector) # Return both mean and median as a list return(list(mean = mean_value, median = median_value)) } # Example usage of the mean_and_median function results &lt;- mean_and_median(c(12, 19, 21, 14, 09)) # Print the results print(paste(&quot;Mean:&quot;, results$mean, &quot;, Median:&quot;, results$median)) ## [1] &quot;Mean: 15 , Median: 14&quot; ________________________________________________________________________________ "],["group-manipulation.html", "Chapter 3 Group Manipulation 3.1 Apply Family 3.2 Aggregate Plyr 3.3 Data Reshaping 3.4 Hands-on Exercise", " Chapter 3 Group Manipulation Group manipulation in R refers to the process of grouping data based on certain categories and then performing operations based on each group separately. This is useful when you want to summarize, analyze or transform subsets of your data independently. In simple terms, group manipulation involves splitting the data into groups, applying a function to each group, and then combining the results. We will explore different methods designed by researchers for group manipulation. They are group manipulation using; The apply family, The aggregate from plyr package, Data reshaping 3.1 Apply Family The apply family in R is a collection of functions that helps you apply operations to data structures like vectors, lists, matrices and data frames in a more efficient way than using loops. Think of these functions as a way to give commands to your data in bulk, telling each piece what to do without repeating yourself. Let’s make this fun! Imagine you’re running a café, and you have tables (rows of data) with customer orders (columns of data). You want to calculate the total for each table or find out how much each customer spent on average. The apply family is like hiring a helper who goes to each table and collects information without you having to ask each customer individually! Lets have a quick overview of the members of the apply family; apply() - Works with matrices or data frames, applying a function to rows or columns. lapply() - Loops over elements in a list, applying a function to each element and returning a list. sapply() - Similar to lapply, but it returns a vector or matrix when possible. tapply() - Applies a function over subsets of data, especially useful for factors or groups. mapply() - Applies a function to multiple arguments simultaneously. Try it: Here is the apply family in action using the built-in R data set that contains information about flowers. Use apply to calculate the mean of each column in the iris data set at once(No need of specifying the columns) # Load and view the first few rows of the iris data set data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # Calculate the mean of each numeric column col_means &lt;- apply(iris[, 1:4], 2, mean) print(col_means) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.843333 3.057333 3.758000 1.199333 The 2 in apply means “apply the function to columns” and the mean was used to find the average of each column. This is simple as asking a helper to calculate the the average for all types of flowers for each characteristic (sepal length, petal length, etc.). Let’s repeat the same for a each row, instead of argument value 2 we will put argument value 1 in the second position. row_means &lt;- apply(iris[, 1:4], 1, mean) # Calculate the mean for each row head(row_means, 15) # Show the first fifteen averages of the row ## [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500 ## [13] 2.325 2.125 2.800 Now lets use the lapply function to find the range for each numeric column. This function applies to each element and returns a list. No need to specify if its a column or a row # Calculate the range of each numeric column in the iris dataset column_ranges &lt;- lapply(iris[, 1:4], range) print(column_ranges) ## $Sepal.Length ## [1] 4.3 7.9 ## ## $Sepal.Width ## [1] 2.0 4.4 ## ## $Petal.Length ## [1] 1.0 6.9 ## ## $Petal.Width ## [1] 0.1 2.5 Repeating the function with mean function instead of the range function. # Calculate the mean of each numeric column in the iris dataset col_means &lt;- lapply(iris[, 1:4], mean) print(col_means) ## $Sepal.Length ## [1] 5.843333 ## ## $Sepal.Width ## [1] 3.057333 ## ## $Petal.Length ## [1] 3.758 ## ## $Petal.Width ## [1] 1.199333 You see! lapply function works column wise instead of row wise when working with data frames. Lets create a function that will add 10 to the input value and use the lapply function to work on a vector. # Create a vector current_ages &lt;- c(21, 43, 12, 56, 32) # Create a function that adds 10 to an input value add_10 &lt;- function(value){ return(value + 10) } # Test the function add_10(27) ## [1] 37 # Apply the function to vector ages ages_10_years_later &lt;- lapply(current_ages, add_10) ages_10_years_later # Show the result ## [[1]] ## [1] 31 ## ## [[2]] ## [1] 53 ## ## [[3]] ## [1] 22 ## ## [[4]] ## [1] 66 ## ## [[5]] ## [1] 42 It returns a list with values in the vector current_ages add 10 to each value. The sapply() function works similarly to lapply(), but it tries to simplify the output. If possible, it will return a vector or matrix instead of a list. Let`s calculate the variance for each numeric column; # Calculate the variance for each numeric column col_variance &lt;- sapply(iris[, 1:4], var) print(col_variance) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 0.6856935 0.1899794 3.1162779 0.5810063 Remember that we created a function add_10 that adds 10 to the current ages of the clients. Lets repeat the same using the sapply function instead of lapply function. # Calculate the variance for each numeric column ages_10_years_later &lt;- sapply(current_ages, add_10) print(ages_10_years_later) ## [1] 31 53 22 66 42 It is now evident that sapply has a simpler output than the lapply function. The tapply() function applies a function to subsets of data grouped by a factor (e.g., species in our case). Let’s calculate the average sepal length for each species: # Calculate the average Sepal.Length for each Species avg_sepal_by_species &lt;- tapply(iris$Sepal.Length, iris$Species, mean) print(avg_sepal_by_species) ## setosa versicolor virginica ## 5.006 5.936 6.588 This is like sending your helper to collect the sepal lengths for each species separately, and then calculating the average for each group. Finally the mapply() function is useful when you want to apply a function to multiple sets of arguments at once. Let’s calculate the sum of Sepal.Length and Sepal.Width for each row: # Sum Sepal.Length and Sepal.Width for each row sepal_sum &lt;- mapply(sum, iris$Sepal.Length, iris$Sepal.Width) head(sepal_sum) ## [1] 8.6 7.9 7.9 7.7 8.6 9.3 This function adds the sepal length and width for each flower row by row. It’s like your helper asking every customer for two values and summing them up together. Practical Exercise Now it’s time to test your skills! Use apply() to calculate the maximum for each column in the iris data set. Use lapply() to find the summary statistics (use the summary() function) for each numeric column in the iris data set. Use tapply() to find the average petal width for each species in the iris data set. Solution Use apply() to calculate the maximum for each column in the iris data set. max_values &lt;- apply(iris[, 1:4], 2, max) print(max_values) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 7.9 4.4 6.9 2.5 Use lapply() to find the summary statistics (use the summary() function) for each numeric column in the iris data set. sum_stats &lt;- lapply(iris[,1:4], summary) print(sum_stats) ## $Sepal.Length ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 ## ## $Sepal.Width ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 2.800 3.000 3.057 3.300 4.400 ## ## $Petal.Length ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.600 4.350 3.758 5.100 6.900 ## ## $Petal.Width ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.100 0.300 1.300 1.199 1.800 2.500 Use tapply() to find the average petal width for each species in the iris data set. # Calculate the average Petal.Width for each Species avg_petal_width_by_species &lt;- tapply(iris$Petal.Width, iris$Species, mean) print(avg_petal_width_by_species) ## setosa versicolor virginica ## 0.246 1.326 2.026 ________________________________________________________________________________ 3.2 Aggregate Plyr The aggregate() function from plyr package is a powerful tool for grouping and summarizing data in R. This is similar to the SQL GROUP BY command or the tapply() that we have discussed above. The difference is that aggregate() allows to summarize data based on one or more grouping factors. Try it! Let’s explore an example using the built-in mtcars data set to show how to use the aggregate() from the plyr package. The plyr package can be installed by: install.packages(&quot;plyr&quot;) Lets start library(plyr) ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize # Load the data set data(&quot;mtcars&quot;) # Use aggregate to find the average &#39;mpg&#39; (miles per gallon) grouped by the number of cylinders (&#39;cyl&#39;) avg_mpg_by_cyl &lt;- aggregate(mpg ~ cyl, data = mtcars, FUN = mean) avg_mpg_by_cyl ## cyl mpg ## 1 4 26.66364 ## 2 6 19.74286 ## 3 8 15.10000 If we break done the code; mpg ~ cyl tells R to calculate the average mpg(dependent variable) for each unique value of cyl(grouping factor). data = mtcars specifies the data set. FUN = mean applies the mean function to compute the average mpg for each group of cyl. We have just calculated the average mpg (miles per gallon) grouped by the number of cyl(cylinders). Let’s make it a little bit more complex by grouping with multiple variables and summarize multiple columns as well. We will calculate the mean horsepower(hp) and the weight(wt) by the number of cylinders(cyl) and the number of transmission(am). # Use aggregate to find the mean hp and wt by cylinders and transmission type avg_hp_wt_by_cyl_am &lt;- aggregate(cbind(hp, wt) ~ cyl + am, data = mtcars, FUN = mean) avg_hp_wt_by_cyl_am ## cyl am hp wt ## 1 4 0 84.66667 2.935000 ## 2 6 0 115.25000 3.388750 ## 3 8 0 194.16667 4.104083 ## 4 4 1 81.87500 2.042250 ## 5 6 1 131.66667 2.755000 ## 6 8 1 299.50000 3.370000 If we breakdown the code; cbind(hp, wt) allows you to summarize multiple columns (hp and wt). cyl + am groups the data by the number of cylinders and the transmission type (am = 0 for automatic, 1 for manual`). The argument FUN defines the function to be used here therefore, FUN = mean calculates the mean values for hp and wt for each group of cyl and am. Practical Exercise Try using the aggregate() with the iris data set to find the mean sepal length (Sepal.Length) and petal length(Petal.Length) for each species. Solution library(plyr) # Load the iris data set data(iris) # Calculate the averages as per the instructions avg_sepal_petal_by_species &lt;- aggregate(cbind(Sepal.Length, Petal.Length) ~ Species, data = iris, FUN = mean) avg_sepal_petal_by_species ## Species Sepal.Length Petal.Length ## 1 setosa 5.006 1.462 ## 2 versicolor 5.936 4.260 ## 3 virginica 6.588 5.552 ________________________________________________________________________________ 3.3 Data Reshaping Data reshaping is the process of transforming the layout or structure of a data set without changing the actual data. You typically reshape data to suit different analyses, visualizations, or reporting formats. Common operations for reshaping include pivoting data between wide and long formats. Wide format: Each subject(row) has its own columns for measurements at different time points or categories. Long format: The data has one measurement per row, making it easier to analyze in some cases, especially with repeated measures. In R, the most common function for reshaping data include; pivot_longer() and pivot_wider() from the tidyr package. melt() and dcast() from the reshape2 package. Try it! Let’s have some fun by working on the mtcars data set where we will demonstrate reshaping between wide and long formats Step 1: Inspect the Data The mtcars data set is already in a wide format where each row represents a car, and columns represent different variables for instance mpg, cyl, hp. data(mtcars) # Load the data set # First few records of the data set head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Step2: Converting from Wide to Long Format We will use the pivot_longer() function from the tidyr package to convert the data set from wide to long format. In this case, we will shape the mpg, hp and wt columns into a longer format making it easier to work with. library(tidyr) # Reshape the data from wide to long format mtcars_long &lt;- mtcars %&gt;% pivot_longer(cols=c(mpg, hp, wt), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # View the respaed data head(mtcars_long) ## # A tibble: 6 × 10 ## cyl disp drat qsec vs am gear carb variable value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 6 160 3.9 16.5 0 1 4 4 mpg 21 ## 2 6 160 3.9 16.5 0 1 4 4 hp 110 ## 3 6 160 3.9 16.5 0 1 4 4 wt 2.62 ## 4 6 160 3.9 17.0 0 1 4 4 mpg 21 ## 5 6 160 3.9 17.0 0 1 4 4 hp 110 ## 6 6 160 3.9 17.0 0 1 4 4 wt 2.88 If we break down the code; pivot_longer() function moves the selected columns (mpg, hp, wt) into a new “long” format, with eah row representing a unique combination of car characteristics(variable) and their corresponding value. names_to = \"variable\": The variable names (e.g., mpg, hp, wt) are moved to a column named “variable”. values_to = \"value\": The data for each variable is placed in a column named \"value\". Also, data in long format can be converted to a wide format. The pivot_wider function from dplyr gets the work done. Try it! Lets put the pivot_wider function into practice. We will convert the ntcars_long data set that we just recently generated to a wider format. # Reshape from long to wide format mtcars_wide &lt;- mtcars_long %&gt;% pivot_wider(names_from = &quot;variable&quot;, values_from = &quot;value&quot;) # View the reshaped data head(mtcars_wide) ## # A tibble: 6 × 11 ## cyl disp drat qsec vs am gear carb mpg hp wt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 160 3.9 16.5 0 1 4 4 21 110 2.62 ## 2 6 160 3.9 17.0 0 1 4 4 21 110 2.88 ## 3 4 108 3.85 18.6 1 1 4 1 22.8 93 2.32 ## 4 6 258 3.08 19.4 1 0 3 1 21.4 110 3.22 ## 5 8 360 3.15 17.0 0 0 3 2 18.7 175 3.44 ## 6 6 225 2.76 20.2 1 0 3 1 18.1 105 3.46 If we break down the code; pivot_wider() converts the long format back into the wide format, with separate columns for each variable (mpg, hp, wt). names_from = \"variable\": Moves the unique values from the \"variable” column into their own columns (e.g., mpg, hp, wt). values_from = \"value\": Populates the new columns with values from the “value” column. Practical Exercise Use the pivot_longer() function to convert the iris dataset (which contains measurements for different flower features) into a long format. Focus on converting the numeric columns like Sepal.Length and Sepal.Width. Then, use pivot_wider() to convert it back to a wide format. Solution Convert to long format library(tidyr) # Load the data data(iris) # Load the iris dataset and reshape it iris_long &lt;- iris %&gt;% pivot_longer(cols = starts_with(&quot;Sepal&quot;), names_to = &quot;feature&quot;, values_to = &quot;measurement&quot;) # View the reshaped data head(iris_long) ## # A tibble: 6 × 5 ## Petal.Length Petal.Width Species feature measurement ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.4 0.2 setosa Sepal.Length 5.1 ## 2 1.4 0.2 setosa Sepal.Width 3.5 ## 3 1.4 0.2 setosa Sepal.Length 4.9 ## 4 1.4 0.2 setosa Sepal.Width 3 ## 5 1.3 0.2 setosa Sepal.Length 4.7 ## 6 1.3 0.2 setosa Sepal.Width 3.2 Back to wide # Now reshape it back to wide format iris_wide &lt;- iris_long %&gt;% pivot_wider(names_from = &quot;feature&quot;, values_from = &quot;measurement&quot;) # View the reshaped data head(iris_wide) ## # A tibble: 6 × 5 ## Petal.Length Petal.Width Species Sepal.Length Sepal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; ## 1 1.4 0.2 setosa &lt;dbl [8]&gt; &lt;dbl [8]&gt; ## 2 1.3 0.2 setosa &lt;dbl [4]&gt; &lt;dbl [4]&gt; ## 3 1.5 0.2 setosa &lt;dbl [7]&gt; &lt;dbl [7]&gt; ## 4 1.7 0.4 setosa &lt;dbl [1]&gt; &lt;dbl [1]&gt; ## 5 1.4 0.3 setosa &lt;dbl [3]&gt; &lt;dbl [3]&gt; ## 6 1.5 0.1 setosa &lt;dbl [2]&gt; &lt;dbl [2]&gt; 3.4 Hands-on Exercise Solution ________________________________________________________________________________ "],["general-statistics.html", "Chapter 4 General Statistics 4.1 Measure of Spread and Central Tendency 4.2 Data Visualization 4.3 Inferential Statistics", " Chapter 4 General Statistics In this course, we will require the ecommerce data set that can be downloaded from here. Lets load the data set ecommerce &lt;- read.csv(&quot;data/ecommerce.csv&quot;) # First few rows of the data set head(ecommerce) ## CID TID Gender Age.Group Purchase.Date Product.Category ## 1 943146 5876328741 Female 25-45 30/08/2023 20:27:08 Electronics ## 2 180079 1018503182 Male 25-45 23/02/2024 09:33:46 Electronics ## 3 337580 3814082218 Other 60 and above 06/03/2022 09:09:50 Clothing ## 4 180333 1395204173 Other 60 and above 04/11/2020 04:41:57 Sports &amp; Fitness ## 5 447553 8009390577 Male 18-25 31/05/2022 17:00:32 Sports &amp; Fitness ## 6 200614 3994452858 Male 18-25 12/07/2021 15:10:27 Clothing ## Discount.Availed Discount.Name Discount.Amount..INR. Gross.Amount ## 1 Yes FESTIVE50 64.30 725.304 ## 2 Yes SEASONALOFFER21 175.19 4638.992 ## 3 Yes SEASONALOFFER21 211.54 1986.373 ## 4 No 0.00 5695.613 ## 5 Yes WELCOME5 439.92 2292.651 ## 6 Yes FESTIVE50 127.01 3649.397 ## Net.Amount Purchase.Method Location ## 1 661.004 Credit Card Ahmedabad ## 2 4463.802 Credit Card Bangalore ## 3 1774.833 Credit Card Delhi ## 4 5695.613 Debit Card Delhi ## 5 1852.731 Credit Card Delhi ## 6 3522.387 Credit Card Delhi We will explore the fundamentals of statistics using the data set that is packed with real-life insights on customers and their purchasing behavior. At the end of this course unit you will be able to understand; Measure of spread and central tendency Mean(the average) Mode Quartiles Range(IQR, maximum and minimum) Standard deviation and variance Data Visualizations Relationship(scatter plot and heatmaps) Trend(line chart) Distribution(histograms and density plots) Comparison(bar charts) Composition (pie charts) Outliers (boxplots and violin plots) Inferential Statistics Hypothesis testing T-tests( one sample, anova, chi-square) 4.1 Measure of Spread and Central Tendency Central tendency identifies the center or typical value of a data set. Measuring central tendency summarizes the data by identifying skewness, distribution and how the data is robust to outliers. Business calculate the central tendencies like average sales, median customer age to make informed decision-making. Here are some of the statistical concepts used to define central tendecy and spread; Mean Mode Quartiles(Median, upper and lower quartile) Range and IQR Variance and standard deviation a.Arithmetic mean It is also referred to as the average. It is the sum of all values divided by the number of values in the set. R has a function mean to calculate the average. Here is the formula for mean; \\[\\overline{x} ={ \\sum^{n}_{i=1}x_i\\over{n}}\\] Where; \\(\\overline{x}\\) is the mean \\(x_i\\) represent value at position \\(i\\) \\(n\\) is the number(count) of values in a set Lets calculate the average net amount spent on the ecommerce store. # Calculate the mean net amount mean(ecommerce$Net.Amount) ## [1] 2875.95 The average amount spent on the e-commerce store by all the customers was 2875.95 INR. Mode The mode shows the most frequent value, thereby pinpoint the most popular categories in a data set. In the commerce data set we will find the most popular category and age group in the data set. Find the most popular category # Calculate the mode for &#39;ProductCategory&#39; table(ecommerce$Product.Category) ## ## Beauty and Health Books Clothing Electronics ## 8332 2762 10968 16574 ## Home &amp; Kitchen Other Pet Care Sports &amp; Fitness ## 5489 2171 1618 5557 ## Toys &amp; Games ## 1529 which.max(table(ecommerce$Product.Category)) ## Electronics ## 4 Quartiles ____Come back later_____ Range To understand range, we will define, maximum and maximum values. The maximum value is the largest value in a set while the minimum is the smallest value in a set. Range is the difference between the maximum(largest value) and the minimum value in a set. Here is a formula for range; \\[Range = Max - Min\\] Lets create a vector ages that defines the ages of different customers on a retail stores, find the maximum and minimum ages then calculate the range. # Create vector ages ages &lt;- c(21, 27, 55, 13, 87, 51, 33, 64) # Find the maximum max(ages) ## [1] 87 # Find the minimum min(ages) ## [1] 13 # Calculate the range diff(range(ages)) ## [1] 74 Try it! Now its clear what range, lets get into the real-world scenario. We will find the maximum and minimum Gross.Amount spent on the ecommerce store. Finally, we calculate the range of amount spent by customers(Gross.Amount) # Maximum amount spent by customers max(ecommerce$Gross.Amount) ## [1] 8394.826 # Minimum amount spent by customers min(ecommerce$Gross.Amount) ## [1] 136.4543 # Range diff(range(ecommerce$Gross.Amount)) ## [1] 8258.371 Standard deviation and Variance Variance is statistical measure of dispersion that defines how spread the data points are in a data set in relation to the mean of the data set. Standard deviation is the measure of how data is clustered around the mean. It is simply defined to as the square root of variance. Here is the formula of variance; \\[\\sigma = {\\sum(x_i - \\overline{x})^2 \\over{n}}\\] Where; \\(\\sigma\\) is the variance \\(x_i\\) is the value \\(\\overline{x}\\) is the population mean \\(n\\) is the value count Standard is simply the square root of variance, Here is the formula for standard deviation; \\[S = \\sqrt{\\sigma}\\] or \\[S = \\sqrt{{\\sum(x_i - \\overline{x})^2 \\over{n}}}\\] Where \\(S\\) is the standard deviation. Variance and standard deviation can be calculated in R environment using var() and sd() functions respectively. Lets create a vector of weights of the athletes in kilograms and calculate the variance and standard deviation. # Sample vector athlete_weights = c(55, 76, 52, 68, 71, 63, 58, 52, 85, 96) # Calculate variance var(athlete_weights) ## [1] 216.7111 # Calculate the standard deviation sd(athlete_weights) ## [1] 14.72111 Lets put into real world practice and calculate variance and standard deviation of net amount spent on the ecommerce store. # Variance var(ecommerce$Net.Amount) ## [1] 2979517 # Standard deviation sd(ecommerce$Net.Amount) ## [1] 1726.128 Practical Exercise Solution ________________________________________________________________________________ 4.2 Data Visualization Data visualization is the process of using visual elements like chart, graphs or maps to represent data in a way that is easier to understand. Luckily, R has a package, ggplot2, specifically designed to create charts and graphs. The package is said to implement the “Grammar of Graphics” that is a conceptual framework for creating graphs by Leland Wilkinson. The package can be installed by; install.packages(&quot;ggplot2&quot;) and once the package is installed it is loaded by; library(ggplot2) Lets create simple charts with the ggplot2 package. Scatter plots Scatter plot is used to show the numerical relationship between two or more. Lets create a simple plot. # Sample data df &lt;- data.frame( x = rnorm(100), y = rnorm(100) ) # Creating a scatter plot ggplot(df, # data aes(x = x, y = y)) + #aesthetics geom_point() + #geometrics labs(title = &quot;Scatter Plot Example&quot;, x = &quot;X Values&quot;, y = &quot;Y Values&quot;) + theme_minimal() Try it! Lets make it more interesting by exploring the real world scenario where we will visualize the relationship between Discount and the Net Amount on the ecommerce store. # Creating a scatter plot ggplot(ecommerce[1:100, ], # first 100 rows of the data aes(x = Net.Amount, y = Discount.Amount..INR.)) + #aesthetics geom_point() + #geometrics labs(title = &quot;Relationship between Net Amount and the Discount&quot;, x = &quot;Net Amount&quot;, y = &quot;Discount&quot;) + theme_minimal() The scatter chart above shows that there is no clear relationship between the Net Amount spent and the discount offered since at every spending(low, medium or high) had low or high discount. High spending did not necessarily guarantee low or high discount. Line Chart Line Chart is used to show trend(Growth or fall over time). Let create a simple line chart to show this; # Sample data dates &lt;- c(&quot;2023-10-01&quot;, &quot;2023-10-05&quot;, &quot;2023-10-10&quot;, &quot;2023-10-15&quot;, &quot;2023-10-20&quot;, &quot;2023-10-25&quot;, &quot;2023-10-30&quot;, &quot;2023-11-04&quot;, &quot;2023-11-09&quot;, &quot;2023-11-14&quot;) scores &lt;- c(75, 82, 88, 85, 90, 87, 92, 95, 98, 100) # Create a data frame df &lt;- data.frame( date = as.Date(dates), scores = scores ) # Plot a scatter plot ggplot(df, # data aes(x=date, y=scores)) + geom_line()+ labs( title = &quot;Scores progression from October to November&quot;, x = &quot;Date&quot;, y = &quot;Scores&quot; ) + theme_minimal() There has a been a gradual increase of scores from October to November. Try it! Lets put this into a real world scenario. We will plot the Gross Amount spent by Customers over the years at the ecommerce store. # Convert the purchase date to date type ecommerce$Purchase.Date &lt;- as.Date(ecommerce$Purchase.Date) # Plot a scatter plot ggplot(ecommerce[1:100, ], # data aes(x=Purchase.Date, y=Gross.Amount)) + geom_line()+ labs( title = &quot;Gross Amount spent over time&quot;, x = &quot;Purchase Date&quot;, y = &quot;Gross Amount&quot; ) + theme_minimal() There has been gradual spikes of rise and fall of the amount spent on the store. Histograms and Density Plots Histograms and Density plots are used to show the distribution of continuous variables. Histogram is visually similar to the bar chart however it is used show frequency and distribution across a list-like data set(vectors, lists, sets, arrays, etc) that stores continuous numeric values. The count of observation within a certain range of values are displayed. Lets create a vector of random 100 ages and plot the data to a histogram. # Generate random 1000 ages between 0 and 100 set.seed(42) ages &lt;- sample(0:100, 1000, replace = TRUE) # Create a data frame to use with ggplot age_data &lt;- data.frame(Age = ages) # PLOTTING # Create the histogram using ggplot ggplot(age_data, aes(x = Age)) + geom_histogram(binwidth = 5, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs( title = &quot;Histogram of Randomly Generated Ages&quot;, x = &quot;Age&quot;, y = &quot;Frequency&quot;) + theme_minimal() The bins are groups of ages ranging 5 years Try it! Lets put his into action and plot the distribution of Discount offered on the ecommerce store # Create the histogram using ggplot ggplot(ecommerce, aes(x = Discount.Amount..INR.)) + geom_histogram(bins = 10, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs( title = &quot;Histogram of Discount offered&quot;, x = &quot;Discount&quot;, y = &quot;Frequency&quot;) + theme_minimal() The distribution is right skewed and most of the customers in the store were offered very low discount for their purchases. Check it later and speak about density plots Bar Charts Bar charts are used to represent both categorical and numeric data in form of rectangular bars. The length/height of each category represents its numeric value. It may corresponds to either length, count, age or any other numerical value. Bar charts are used when;- Comparing categorical data Visualizing summarized data for instance aggregated sum, average or any other summary statistics. Showing frequency or count for instance representing the number of products sold per each category. Ranking data. Bar charts can effectively represents ranks especially in descending/ascending order for instance ranking the life expectancy of different countries. Other type of complex bar charts like stacked bar charts can be used to compare part-to-whole relationships. There are many more uses of bar charts however there are some use cases where bar charts are not preferred like when working with continuous data, scatter and line charts are more befitting. Also, bar charts are not appropriate where data has too many categories, heatmaps will do better. To create a simple bar chart using ggplot2, we use geom_bar to define that its a bar chart. # Sample data df &lt;- data.frame( category = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), value = c(23, 17, 35, 10) ) ## The data set above will be used to create a bar chart # Creating a bar chart ggplot(df, aes(x = category, y = value)) + geom_bar(stat = &quot;identity&quot;) + labs( title = &quot;Value by Category&quot;, x = &quot;Category&quot;, y = &quot;Value&quot;) + theme_minimal() From the bar chart above, Category “C” has the largest value while category “D” has the lowest value. Try it! Lets create a bar chart of a real world scenario. We will calculate the average Gross amount spent by different age groups in the ecommerce store. library(plyr) # Aggregate by mean PurchasebyAge &lt;- aggregate(Gross.Amount ~ Age.Group, data = ecommerce, FUN = mean ) # Plot a bar chart ggplot(PurchasebyAge, aes(x = Age.Group, y = Gross.Amount)) + geom_bar(stat = &quot;identity&quot;, fill=&quot;blue&quot;, color=&quot;white&quot;) + labs( title = &quot;Average Gross Amount by Age Group&quot;, x = &quot;Age Group&quot;, y = &quot;Average Gross Amount&quot;) + theme_minimal() All the age group had an almost equal average spending on the ecommerce store. What if we calculated the total spending for each age group? That would be interesting! Lets calculate and plot a bar chart to represent that. # Aggregate by sum PurchasebyAge &lt;- aggregate(Gross.Amount ~ Age.Group, data = ecommerce, FUN = sum ) # Plot a bar chart ggplot(PurchasebyAge, aes(x = Age.Group, y = Gross.Amount)) + geom_bar(stat = &quot;identity&quot;, fill=&quot;blue&quot;, color=&quot;white&quot;) + labs( title = &quot;Total Gross Amount by Age Group&quot;, x = &quot;Age Group&quot;, y = &quot;Total Gross Amount&quot;) + theme_minimal() Age 25 - 45 had the highest total spending in the ecommerce store while age of 60 and above had the least total spending. Pie Charts ______Explain about pie charts________ Boxplots and Violin Plots _______Explain about outliers using box plots_______ Practical Exercise Solution ________________________________________________________________________________ 4.3 Inferential Statistics 4.3.1 Introduction to Hypothesis Testing 4.3.1.1 Concept of Hypothesis Testing Hypothesis testing is a type of statistical analysis that is used to make assumptions of a population based on a sample of data. It is particularly used to find the relationship between two variables(populations). A real life example of hypothesis testing is that a teacher may assume that 60% of the students come from a middle-class family. There are two types of hypothesis; Null hypothesis(\\(H_0\\)) Alternate hypothesis (\\(H_1\\) or \\(H_a\\)) Null hypothesis is states that there is no effect or no difference(\\(\\mu = 0\\)). For instance there is no effect of standards of living to college admissions. Alternate hypothesis is the opposite and contradicts the null hypothesis. It provide evidence for what the statistician is trying to find(\\(\\mu \\neq 0\\)). In this case, the standards of living have an effect on college admissions. The important aspects before conducting hypothesis testing are;- Significance level. It is the probability of rejecting the null hypothesis when it is actually true. P-Value is the probability of obtaining a test statistic at least as extreme as the one observed, given the null hypothesis is true. Most hypothesis testing projects are set at 0.05. Less than 0.05(or the set value) indicates the null the test is statistically significant and the null hypothesis should be rejected. Otherwise, the test is statistically insignificant and the null hypothesis is not rejected. Test statistic also called T-statistic is a standardized value calcluated during a hypothesis test. It cab z-test or a t-test. -Decision rule is based on the calculated p-value and the significant level. In a hypothesis test where the significant and the p-value is 0.03444 the null hypothesis is not rejected. Now that you are familiar with the hypothesis testing aspects, take the following steps to perform hypothesis testing;- Formulating the hypothesis by defining the null and alternate hypothesis. Collect and analyze the data. Choose a significant level(\\(a\\)) and calculate the p-value. Make a decision by comparing the p-value to the significant level. Conclude your analysis results. 4.3.1.2 T-tests One-Sample t-test One sample t-test is a statistical method used to find if the mean of a sample is different from the population(or preassumed) mean. It is based on the t-distribution(most observations fall close to the mean, and the rest of the observations make up the tails on either side) and is commonly used when dealing with small sample sizes. One sample t-test is especially performed where the population standard deviation is unknown. Below is the formula for one sample t-test \\[t={{\\overline{X}-\\mu}\\over s / \\sqrt{n}}\\] where; \\(t\\): the one sample t-test value. t-test statistic \\(n\\): the number of the observations in the sample \\(\\overline{X}\\): is the sample mean \\(s\\): standard deviation of the sample \\(\\mu\\): Hypothesized population mean The result \\(t\\), simply measures how many standard errors the sample mean is away from the hypothesized population mean. Before conducting t-test, there is a need to establish the null(H0) and alternate hypothesis(Ha) where; Null Hypothesis(H0): There is no significant difference between the sample mean and the population(hypothesized) mean. ALternate Hypothesis(Ha): There is a significant difference between the sample mean and the population mean. P-value is the probability value that tells you how likely is that your data could have occurred under null hypothesis. In our case a p-value of below 0.05 is considered to be statistically significant and the null value is rejected. The vice versa is true Lets perform a t-test using R; We will generate sample data # Set seed for reproducibility set.seed(123) # Generate random student marks (out of 100) student_marks &lt;- rnorm(30, mean = 65, sd = 10) # Display the first few marks head(student_marks) ## [1] 59.39524 62.69823 80.58708 65.70508 66.29288 82.15065 Perform the t-test # Conduct one-sample t-test t_test_result &lt;- t.test(student_marks, mu = 70) # Display the t-test result print(t_test_result) ## ## One Sample t-test ## ## data: student_marks ## t = -3.0546, df = 29, p-value = 0.004797 ## alternative hypothesis: true mean is not equal to 70 ## 95 percent confidence interval: ## 60.86573 68.19219 ## sample estimates: ## mean of x ## 64.52896 Practical exercise Conduct one sample t-test on the sepal length of the setosa iris. The pre-assumed mean is 5.84 units. Solution Null Hypothesis(\\(H_0\\)): There is no significant difference between the setosa sepal length mean and the pre-assumed mean Alternate Hypothesis(\\(H_a\\)): There is a significant difference between the setosa sepal length and the pre-assumed mean Note: If the pre-assumed mean is not given in the question therefore the whole population mean is used(as the pre-assumed mean). In this case, the setosa sepal length mean of the whole iris species. library(dplyr) # Load the data data(iris) # Get the setosa sepal length setosa &lt;- iris %&gt;% filter(Species==&quot;setosa&quot;) setosa.sepal.length &lt;- setosa$Sepal.Length # Calculate one-sample ttest ## Set the pre-assumed mean pre_assumed_mean &lt;- 5.84 # as in the question t_test_result &lt;- t.test(setosa.sepal.length, mu = pre_assumed_mean) print(t_test_result) ## ## One Sample t-test ## ## data: setosa.sepal.length ## t = -16.73, df = 49, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 5.84 ## 95 percent confidence interval: ## 4.905824 5.106176 ## sample estimates: ## mean of x ## 5.006 The p-value is much below 0.05, therefore the null hypothesis is rejected. It is concluded that there is a significant difference between the sepal length mean and the pre-assumed mean ________________________________________________________________________________ Two-Sample t-test Unlike one sample t-test where a sample population is tested against a pre-assumed mean, the Tow-sample t-test determines if there is a significant difference between the means of two independent populations. The practical application of two-sample t-test can be when comparing the test scores of two classes. This helps the statistician to understand if one class did better than the other one or it’s just a matter of luck. These are the prerequisites before conducting a two-sample t-test; The groups contain separate data with a similar distribution. The two populations have a normal(typical bell-curve) distribution. The two sample populations have a similar variations The two sample t-test is calculated by; Where; \\(\\overline{x}_1\\) and \\(\\overline{x}_2\\) are the mean of the first sample and the second sample respectively \\(s_{1}\\) and \\(s_{2}\\) are the standard deviation of sample 1 and sample 2 respectively \\(n_1\\) and \\(n_2\\) are the sample sizes of the first and second sample respectively. Let create a random population of student scores for two classes and perform two-sample t-test in R; # Generate the population sample set.seed(123) group_A_scores &lt;- rnorm(25, mean = 75, sd = 8) # Group A group_B_scores &lt;- rnorm(25, mean = 80, sd = 10) # Group B # Display the first few scores of each group head(group_A_scores) ## [1] 70.51619 73.15858 87.46967 75.56407 76.03430 88.72052 head(group_B_scores) ## [1] 63.13307 88.37787 81.53373 68.61863 92.53815 84.26464 Performing the two sample t-test. Lets set the confidence level to 95%(0.95) ttest_result = t.test(group_A_scores, group_B_scores, alternative = &quot;two.sided&quot;, mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95) ttest_result ## ## Welch Two Sample t-test ## ## data: group_A_scores and group_B_scores ## t = -2.6403, df = 46.312, p-value = 0.01125 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11.080984 -1.495049 ## sample estimates: ## mean of x mean of y ## 74.73336 81.02137 t-value = -2.6403: This indicates the difference between the means of the two groups, in terms of standard errors. A higher absolute value suggests a larger difference. Degrees of freedom (df) = 46.312: This reflects the sample size and variability in the data. p-value = 0.01125: Since the p-value is less than 0.05, we reject the null hypothesis. This suggests that the difference in means between Group A and Group B is statistically significant. 95% confidence interval: (-11.08, -1.50): This indicates that we are 95% confident that the true difference in means lies between -11.08 and -1.50. Mean of x (Group A) = 74.73, Mean of y (Group B) = 81.02: The average score of Group B is higher than Group A. In summary, the test shows a significant difference between the means of the two groups, with Group B having higher scores Practical exercise Using the iris data set, compare the petal length of the versicolor and virginica species using two-sample t-test. Interpret the results Solution Lets formulate the hypothesis; Null Hypothesis(\\(H_0\\)): There is no significant difference between the mean of virginica and the mean of versicolor Alternate Hypothesis(\\(H_a\\)): There is a significant difference between the mean of virginica and the versicolor library(dplyr) # Load the data data(iris) # Retrieve the data for the two species ## Virginica petal length virginica &lt;- iris%&gt;% filter(Species==&quot;virginica&quot;) virginica.petal.length &lt;- virginica$Petal.Length ## Versicolor petal length versicolor &lt;- iris%&gt;% filter(Species==&quot;versicolor&quot;) versicolor.petal.length &lt;- versicolor$Petal.Length # Conduct two-sample t-test ttest_result = t.test(virginica.petal.length, versicolor.petal.length, alternative = &quot;two.sided&quot;, mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95) ttest_result ## ## Welch Two Sample t-test ## ## data: virginica.petal.length and versicolor.petal.length ## t = 12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1.08851 1.49549 ## sample estimates: ## mean of x mean of y ## 5.552 4.260 With a p-value of less than 0.05, the null hypothesis is rejected. It is concluded that there is a significant difference between the mean of virginica species and the mean of versicolor species. ________________________________________________________________________________ 4.3.1.3 ANOVA (Analysis of Variance) ANOVA is a statistical test used to analyze the difference between the means of more than two groups. This is different from the ttest that analyzes one or two groups, it uses F test to find the statistical significance. Multiple means are compared at once and if one mean is different the hypothesis is rejected. The F test compares the variance in each group from the overal group variance. An practical example of ANOVA is where a farmer wants to test the effect of three different fertilizers on the crop yield. The difference in the crop yield will be calculated. Before conducting ANOVA, the following assumptions are made; Independence of observations: the data was collected using statistically valid sampling methods and there are no hidden relationships among the observations. Normal distribution: the dependent variable should follow a normal distribution. Homogeinity of variance: All the groups being tested should have similar variations. Lets calculate the ANOVA using the the crop yield data set. The fertilizer are in three categories; 1, 2 and 3 # Load the data set crop_df &lt;- read.csv(&quot;data/cropdata.csv&quot;) head(crop_df) # view the first few rows of the data set ## density block fertilizer yield ## 1 1 1 1 177.2287 ## 2 2 2 1 177.5500 ## 3 1 3 1 176.4085 ## 4 2 4 1 177.7036 ## 5 1 1 1 177.1255 ## 6 2 2 1 176.7783 # Calculate one way ANOVA anova &lt;- aov(yield ~ fertilizer, data = crop_df) anova ## Call: ## aov(formula = yield ~ fertilizer, data = crop_df) ## ## Terms: ## fertilizer Residuals ## Sum of Squares 5.74322 36.21101 ## Deg. of Freedom 1 94 ## ## Residual standard error: 0.6206638 ## Estimated effects may be unbalanced The ANOVA output provides insights into the variation in crop yield explained by the fertilizer type. Here’s a detailed breakdown of the results: Sum of Squares (fertilizer) = 5.74322: This value represents the variation in crop yield that can be attributed to the different types of fertilizers used in the experiment. In this case, 5.74322 units of the total variation are explained by fertilizer differences. Sum of Squares (Residuals) = 36.21101: This is the unexplained variation in crop yield, also known as the error term. This shows how much of the variation is due to factors not accounted for in the model, such as environmental factors or random error. Degrees of Freedom (fertilizer) = 1: There is only 1 degree of freedom for the fertilizer factor, which means there was a comparison between two groups (likely two fertilizer types or one fertilizer versus a control). Degrees of Freedom (Residuals) = 94: There are 94 degrees of freedom associated with the residuals. This is related to the total number of observations minus the number of groups being compared. In this case, the large degrees of freedom indicate a sizable data set. Residual Standard Error = 0.6206638: This value represents the typical deviation of the observed yield values from the predicted values, given the current model. A lower residual standard error suggests a better fit of the model to the data, though this value needs to be interpreted in context. The results show that the fertilizer type explains some of the variation in crop yield (Sum of Squares = 5.74322), while a larger portion remains unexplained (Sum of Squares of Residuals = 36.21101). To fully interpret the significance of this effect, a p-value and F-statistic would typically be calculated, but these are not provided here. Additionally, the residual standard error (0.6206638) gives an indication of the spread of the data around the predicted values, but more information would be needed to assess the strength of the model’s fit. In conclusion, while the fertilizer has some effect on crop yield, the overall variability and potential unbalanced data need further exploration for a complete understanding. Practical exercise Perform ANOVA on the sepal width of the three species in the iris data set and interpret the results. Solution Lets formulate the hypothesis; Null Hypothesis(\\(H_0\\)): There is no siginificant difference among the sepal width of all the iris species. Alternate Hypothesis(\\(H_a\\)): There is a signifincant difference among the sepal width of all the iris species # Load the data data(iris) # Perform the ANOVA anova_result &lt;- aov(Sepal.Width ~ Species, data = iris) # Display the ANOVA table summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 11.35 5.672 49.16 &lt;2e-16 *** ## Residuals 147 16.96 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 it indicates that there are statistically significant differences in the mean Sepal.Width between the species. ________________________________________________________________________________ 4.3.1.4 Chi-Square Test This is a statistical test that determines the difference between the observed and the expected data. It determines if the relationship between two categorical variables is due to chance or a relationship between them. It is calculated by; \\[x_{c}^{2} = \\frac{\\sum(O_{i}-E_{i})}{E_i}\\] Where; \\(c\\) is the degree of freedom. This is a statistical calculation that represents the number of variables that can carry and is calculated to ensure the chi-square tests are statistically valid \\(O\\) is the observed value \\(E\\) is the expected value Lets perform Chi-square on a survey data from the MASS library. The survey data represents data from a survey conducted on students. Null Hypothesis (\\(H_0\\)): The smoking habit is independent of the student’s exercise level ALternate Hypothesis (\\(H_a\\)): The smoking habit is dependent on the exercise level. Load the data # Load the library library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select head(survey) # view the data ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I ## 1 Female 18.5 18.0 Right R on L 92 Left Some Never 173.00 Metric ## 2 Male 19.5 20.5 Left R on L 104 Left None Regul 177.80 Imperial ## 3 Male 18.0 13.3 Right L on R 87 Neither None Occas NA &lt;NA&gt; ## 4 Male 18.8 18.9 Right R on L NA Neither None Never 160.00 Metric ## 5 Male 20.0 20.0 Right Neither 35 Right Some Never 165.00 Metric ## 6 Female 18.0 17.7 Right L on R 64 Right Some Never 172.72 Imperial ## Age ## 1 18.250 ## 2 17.583 ## 3 16.917 ## 4 20.333 ## 5 23.667 ## 6 21.000 Create a contigency table between the Smoke and the Exercise leel. # Create a contingency table with the needed variables. stu_data = table(survey$Smoke,survey$Exer) print(stu_data) ## ## Freq None Some ## Heavy 7 1 3 ## Never 87 18 84 ## Occas 12 3 4 ## Regul 9 1 7 Perform the chi-square test on the stu_data, the contigency table. # applying chisq.test() function print(chisq.test(stu_data)) ## ## Pearson&#39;s Chi-squared test ## ## data: stu_data ## X-squared = 5.4885, df = 6, p-value = 0.4828 From the results, the p-value is 0.4828 which is greater than 0.05 therefore the null hypothesis is not rejected. It’c concluded that the smoking habit is independent of the exercise level since there is weak to now correlation between the Smoke and Exer variables. Finally, lets visualize the results from the contigency table; # Visualize the data with a bar plot barplot(stu_data, beside = TRUE, col = c(&quot;lightblue&quot;, &quot;lightgreen&quot;), main = &quot;Smoking Habits vs Exercise Levels&quot;, xlab = &quot;Exercise Level&quot;, ylab = &quot;Number of Students&quot;) # Add legend separately legend(&quot;center&quot;, legend = rownames(stu_data), fill = c(&quot;lightblue&quot;, &quot;lightgreen&quot;)) You can see from the table, those students who never smoke lead in every exercise level while the heavy smokers are the least in every group. Practical exercise Using the Iris data set, perform a Chi-square test to determine if there is a relationship between two categorical variables: the species (Species) and a new categorical variable that classifies sepal width (Sepal.Length) into categories (e.g., “Short”, “Medium”, “Long”). “Short”: below 3.0 “Medium”: above 3.0 to 3.8 “Long”: above 3.8 Follow the steps below; Create a new variable sepal.Width.Category in the data set by categorizing the Sepal.Width variable into 3 categories: \"Short\", \"Medium\", and \"Long\" (as per the defined ranges). Perform a Chi-square test to see if there’s an association between the new sepal.Width.Category categories and the Species column. Interpret the results of the Chi-square test Solution Load the data set data(iris) Create a new variable sepal.Width.Category in the data set by categorizing the Sepal.Width variable into 3 categories: \"Short\", \"Medium\", and \"Long\" (as per the defined ranges). # Load necessary libraries library(dplyr) # Create a new column &#39;Sepal_Width_Category&#39; based on conditions iris &lt;- iris %&gt;% mutate(sepal.Width.Category = case_when( Sepal.Width &lt; 3.0 ~ &quot;Short&quot;, Sepal.Width &gt;= 3.0 &amp; Sepal.Width &lt;= 3.8 ~ &quot;Medium&quot;, Sepal.Width &gt; 3.8 ~ &quot;Long&quot; )) # Display the first few rows to see the new column head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## sepal.Width.Category ## 1 Medium ## 2 Medium ## 3 Medium ## 4 Medium ## 5 Medium ## 6 Long Perform a Chi-square test to see if there’s an association between the new sepal.Width.Category categories and the Species column. # Create a contigency table sepal_width_data = table(iris$sepal.Width.Category,iris$Species) print(sepal_width_data) ## ## setosa versicolor virginica ## Long 6 0 0 ## Medium 42 16 29 ## Short 2 34 21 # applying chisq.test() function print(chisq.test(sepal_width_data)) ## Warning in chisq.test(sepal_width_data): Chi-squared approximation may be ## incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: sepal_width_data ## X-squared = 50.918, df = 4, p-value = 2.322e-10 Intepret the results ________________________________________________________________________________ 4.3.2 Hands-on Exercises You are required to download the Groundhog Day Forecasts and Temperatures data set from here. Perform one sample t-test on February Average temperature. The pre-assumed mean is 35 Conduct two sample t-test between the North East average temperature for March and the overall March daily temperature Perform ANOVA between the presence of Punxsutawney Phil and the February average temperature Interpret the results Solution Load the data df &lt;- read.csv(&quot;data/GroundhogDayForecastsandTemperaturesdata.csv&quot;) Perform one sample t-test on February Average temperature. The pre-assumed mean is 35 Formulate the hypothesis - Null hypothesis (\\(H_0\\)): There is no significant difference between the February Average Temeprature mean and the pre-assumed mean - Alternate Hypothesis(\\(H_a\\)): There is a significant difference between the February Average Temperature and the pre-assumed mean. one.sample.ttest &lt;- t.test(df$February.Average.Temperature, mu=35) # Display the results print(one.sample.ttest) ## ## One Sample t-test ## ## data: df$February.Average.Temperature ## t = -4.061, df = 122, p-value = 8.671e-05 ## alternative hypothesis: true mean is not equal to 35 ## 95 percent confidence interval: ## 33.21928 34.38641 ## sample estimates: ## mean of x ## 33.80285 The p-value is below 0.05, therefore the is enough evidence to reject the null hypothesis and conclude that there is a significant difference between the February Average Temperature and the pre-assumed mean(35). Conduct two sample t-test between the North East average temperature for March and the overall March daily temperature. Formulate the hypothesis: Null hypothesis(\\(H_0\\)): There is no significant difference between the North East Average Temperature for March and the March daily temparature. ALternate Hypothesis(\\(H_a\\)): There is a significant difference between the North East Average Temperature and the March daily temparature. # Perform two sample ttest ttest_result = t.test(df$March.Average.Temperature..Northeast., df$March.Average.Temperature, alternative = &quot;two.sided&quot;, mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95) ttest_result ## ## Welch Two Sample t-test ## ## data: df$March.Average.Temperature..Northeast. and df$March.Average.Temperature ## t = -21.288, df = 227.4, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.193516 -8.466321 ## sample estimates: ## mean of x mean of y ## 32.36748 41.69740 The p-value is less than 0.05, therefore the null hypothesis is rejected. It is concluded that there is significant difference between the North East Average Temperature for March and overall Average daily Temperature for March Perform ANOVA between the presence of Punxsutawney Phil and the February average temperature Formulate the hypothesis Null Hypothesis(\\(H_0\\)): There is no significant difference between the presence and the absence of Punxsutawney Phil during February based on the Average Temperature Alternate Hypothesis(\\(H_a\\)): There is a significant difference between the presence and the absence of Punxsutawney Phil during February based on the Average Temperature # Perform ANOVA anova_result &lt;- aov(February.Average.Temperature ~ df$Punxsutawney.Phil, data = df) # Display the ANOVA table summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## df$Punxsutawney.Phil 4 92.7 23.18 2.258 0.0669 . ## Residuals 118 1211.3 10.27 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 9 observations deleted due to missingness ________________________________________________________________________________ "],["simple-linear-regression.html", "Chapter 5 Simple Linear Regression 5.1 Basics of Wilkinson-Rogers Notation (y ~ x), Linear Regression 5.2 Scatterplots with Regression Lines, Reading lm() Output 5.3 Confidence Intervals for Regression Coefficients, Testing Coefficients 5.4 Identifying Points in a Plot", " Chapter 5 Simple Linear Regression Welcome to the world of Simple Linear Regression! 🎉 This statistical technique is super handy when you want to explore the relationship between two continuous variables. Essentially, it helps us predict the value of one variable based on the value of another. For example, imagine you want to predict a student’s exam score based on the number of hours they studied. Here, the hours studied are the independent variable (or predictor), and the exam score is the dependent variable (or response). What is Simple Linear Regression? In simple linear regression, we fit a straight line (called the regression line) through the data points. This line is defined by the equation: \\[y = mx + b\\] Where: \\(y\\) is the predicted value (dependent variable). \\(m\\) is the slope of the line (how much \\(y\\) changes for a unit change in \\(x\\)). \\(x\\) is the independent variable. \\(b\\) is the y-intercept (the value of \\(y\\) when \\(x\\) is 0). Let’s use the built-in mtcars data set in R to demonstrate how to perform simple linear regression. Load the data set # Load the mtcars dataset data(mtcars) # View the first few rows of the dataset head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Fit the simple linear regression model that will predict mpg (miles per gallon) based on wt (the weight of the car). # Fit the linear regression model model &lt;- lm(mpg ~ wt, data = mtcars) Get the model summary to get important information about the model we just fitted. # Get the summary of the model summary(model) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 Plot the data and regression line to understand the relationship # Plot the data points plot(mtcars$wt, mtcars$mpg, main = &quot;Simple Linear Regression&quot;, xlab = &quot;Weight of the Car (wt)&quot;, ylab = &quot;Miles Per Gallon (mpg)&quot;, pch = 19, col = &quot;blue&quot;) # Add the regression line abline(model, col = &quot;red&quot;) Predictions can be made based on the data. Lets predict the mpg for car that weighs 3.5 tons # Predict mpg for a car that weighs 3.5 tons new_data &lt;- data.frame(wt = 3.5) predicted_mpg &lt;- predict(model, new_data) print(paste(&quot;Predicted MPG for a car weighing 3.5 tons:&quot;, round(predicted_mpg, 2))) ## [1] &quot;Predicted MPG for a car weighing 3.5 tons: 18.58&quot; 5.1 Basics of Wilkinson-Rogers Notation (y ~ x), Linear Regression 5.2 Scatterplots with Regression Lines, Reading lm() Output 5.3 Confidence Intervals for Regression Coefficients, Testing Coefficients 5.4 Identifying Points in a Plot "],["reproducibility-and-report-with-r-markdown.html", "Chapter 6 Reproducibility and Report with R Markdown 6.1 Key Tools in R for Reproducibility and Reporting 6.2 Creating Reproducible Reports 6.3 Going Beyond: Shiny for Interactive Reporting", " Chapter 6 Reproducibility and Report with R Markdown Reproducibility is one of the core values in data science and R makes it both achievable and easy! Imagine trying to recreate someone’s analysis only to find that you get different results or that they left out crucial steps. Frustrating, right? Reproducibility is the answer—it means you can get the same results every time by following the same steps. Why Reproducibility Matters Trustworthiness: When your results can be replicated, others can trust your analysis. Error Detection: Re-running the same code helps catch mistakes early. Efficiency: With reproducible scripts, you save time if you need to redo parts of your analysis. 6.1 Key Tools in R for Reproducibility and Reporting Let’s dive into the tools that make reproducibility and reporting a breeze in R: R Markdown: This is the gold standard for reproducible reports in R. You can write code, comments, and format it all beautifully in one document. Think of it as combining your code with a notebook-style narrative. Interactive Demo: Create an R Markdown file in RStudio by clicking File &gt; New File &gt; R Markdown…. You can add headers, code chunks, and text. Run Your Code: Run each chunk individually, or click Knit to create a fully formatted report with all your code and outputs embedded. Find here more resources on R markdown. ________remember the resources_______ Setting a Seed fr Consistency: R’s random number generator can be controlled with set.seed(). For instance; set.seed(42) sample(1:100, 5) ## [1] 49 65 25 74 18 This will always produce the same random sample, making your analysis consistent. Code Commenting and Documentation: Clear comments make your analysis easy to understand for others and for yourself. Use comments (#) in your code to describe steps, and include documentation for more complex functions. Below is an example of a comment. # This is a comment 6.2 Creating Reproducible Reports Let’s walk through a simple activity where we create a reproducible report: Set Up Your R Markdown File Open RStudio and create a new R Markdown file. Add a title, your name, and the date. Start with an introduction: “This report explores the relationship between…” _______Add a screenshot______________ Add Your Code and Analysis Insert code chunks for each analysis step. For example, try loading and summarizing the mtcars data set: # Load the data data(mtcars) # Summary of the data set summary(mtcars) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb ## Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :0.0000 Median :4.000 Median :2.000 ## Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :1.0000 Max. :5.000 Max. :8.000 Customize and Style Your Report Add section headers, bold text, and bullet points to organize your report. You can use ggplot2 to add visualizations for a polished look. library(ggplot2) ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + labs(title = &quot;Horsepower vs. Miles per Gallon&quot;) Knit the Report Click the Knit button to render your report into an HTML, PDF, or Word document. Notice how your code, output, and comments are all integrated. 6.3 Going Beyond: Shiny for Interactive Reporting For advanced projects, consider using Shiny to create interactive reports! Shiny apps can run right in your browser and allow users to interact with your data in real time. More details on RShiny will be discussed later on the next topic Reproducibility is a powerful skill—keep practicing, and you’ll quickly see how it enhances your data work! Hands-on Exercises Create an R Markdown file with: A title and introduction explaining your analysis. An example dataset analysis (try using iris or mtcars). A basic visualization. A conclusion summarizing your findings. Solution ________________________________________________________________________________ "],["r-shiny.html", "Chapter 7 R Shiny 7.1 Structure of a Shiny App", " Chapter 7 R Shiny Shiny is a fantastic R package that allows you to easily create interactive web applications (or “apps”) directly from R. In this lesson, we’ll dive right into how to start building Shiny apps. First things first, if you haven’t installed the Shiny package yet, simply open R, make sure you’re connected to the internet, and run the following command: install.packages(&quot;shiny&quot;) Shiny also integrates with another package called bslib, which helps in creating visually appealing user interfaces (UIs). To explore more about it, you can check out its documentation here. Here is an example of an R shiny app The Shiny package comes with several pre-built examples that showcase how Shiny works in action. Each example is a fully functional Shiny app. The Hello Shiny example, for instance, generates a histogram using R’s faithful data set. The histogram’s bin count can be adjusted by the user through a slider, and the app instantly updates based on their selection. This example is perfect for learning the basics of Shiny app structure and building your very first app. To try it out, just run the following commands in R: library(shiny) runExample(&quot;01_hello&quot;) 7.1 Structure of a Shiny App Shiny apps are typically organized in a single script called app.R, which resides in a designated folder (for example, newdir/). You can run the app by executing runApp(\"newdir\"). The app.R file consists of three main components: A User Interface Object A Server Function A Call to the shinyApp Function 7.1.1 User Interface(ui) The user interface (ui) object defines the layout and visual aspects of your app. Below is the ui object used in the Hello Shiny example: library(shiny) library(bslib) ## ## Attaching package: &#39;bslib&#39; ## The following object is masked from &#39;package:utils&#39;: ## ## page # Define UI for app that draws a histogram ---- ui &lt;- page_sidebar( # App title ---- title = &quot;Hello Shiny!&quot;, # Sidebar panel for inputs ---- sidebar = sidebar( # Input: Slider for the number of bins ---- sliderInput( inputId = &quot;bins&quot;, label = &quot;Number of bins:&quot;, min = 1, max = 50, value = 30 ) ), # Output: Histogram ---- plotOutput(outputId = &quot;distPlot&quot;) ) 7.1.2 Server Here i the server function for the Hello Shiny Example: # Define server logic required to draw a histogram ---- server &lt;- function(input, output) { # Histogram of the Old Faithful Geyser Data ---- # with requested number of bins # This expression that generates a histogram is wrapped in a call # to renderPlot to indicate that: # # 1. It is &quot;reactive&quot; and should automatically # re-execute when inputs (input$bins) change # 2. Its output type is a plot output$distPlot &lt;- renderPlot({ x &lt;- faithful$waiting bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = &quot;#007bc2&quot;, border = &quot;white&quot;, xlab = &quot;Waiting time to next eruption (in mins)&quot;, main = &quot;Histogram of waiting times&quot;) }) } At its core, the server function in the Hello Shiny example is quite straightforward. It performs some calculations and generates a histogram based on the specified number of bins. However, you’ll notice that most of the script is wrapped in a call to renderPlot. The comment above the function provides some explanation, but don’t worry if it seems unclear; we will explore this concept in greater detail later. Before you start experimenting with the Hello Shiny app and reviewing its source code, remember that your app.R file should begin with loading the Shiny package and conclude with a call to shinyApp: library(shiny) # See above for the definitions of ui and server ui &lt;- ... server &lt;- ... shinyApp(ui = ui, server = server) While the Hello Shiny app is running, your R session will be occupied and won’t accept other commands. R is actively monitoring the app and processing its reactions. To regain access to your R session, press the escape key or click the stop icon located in the upper right corner of the RStudio console panel. How that you have known how to create an RShiny app, lets create a simple temperature converter that will convert temperature from Celcius to Farenheit. library(shiny) # Define UI for the Temperature Converter app ui &lt;- fluidPage( titlePanel(&quot;Temperature Converter&quot;), sidebarLayout( sidebarPanel( numericInput(&quot;temp_input&quot;, &quot;Temperature:&quot;, value = 0), selectInput(&quot;temp_scale&quot;, &quot;Select scale:&quot;, choices = c(&quot;Celsius to Fahrenheit&quot;, &quot;Fahrenheit to Celsius&quot;)), actionButton(&quot;convert&quot;, &quot;Convert&quot;) ), mainPanel( textOutput(&quot;result&quot;) ) ) ) # Define server logic for the Temperature Converter server &lt;- function(input, output) { observeEvent(input$convert, { if (input$temp_scale == &quot;Celsius to Fahrenheit&quot;) { result &lt;- (input$temp_input * 9/5) + 32 output$result &lt;- renderText({ paste(input$temp_input, &quot;°C =&quot;, round(result, 2), &quot;°F&quot;) }) } else { result &lt;- (input$temp_input - 32) * 5/9 output$result &lt;- renderText({ paste(input$temp_input, &quot;°F =&quot;, round(result, 2), &quot;°C&quot;) }) } }) } # Run the app shinyApp(ui = ui, server = server) Practical Exercise "],["building-r-packages.html", "Chapter 8 Building R Packages 8.1 Introduction 8.2 Prerequisites 8.3 Building the most basic R package 8.4 Making A New R Project 8.5 Adding Documentation 8.6 Uploading and Installing from Github 8.7 Uploading and instaling it from CRAN 8.8 More Info and Additonational Resources", " Chapter 8 Building R Packages 8.1 Introduction In the second chapter we introduced about functions and we later talked about package functions, here we will now focus on how to build those packages. Packages are bundles of code and data to perform created by R users or community to perform a set of goven tasks. In this course you have encountered several packages like dplyr, plyr and ggplot2 and might have installed one or many of them. Packages provide a ready-to-use functions and data sets that produce results faster without the need to write everything from scratch. Here we will discuss how you can create you own package in R. This will give you a deeper appreciation of the packages you rely on daily basis and how they are built. CRAN(The Comprehensive R Archive Network) and GitHub provide a repository where one can host and install packages to their local environments. In this guide, we will walk through how to create a package and host the packages either on GitHub or CRAN. 8.2 Prerequisites Before we jump in, there are a few packages you will want to have ready to help us along the way. We will install devtools and roxygen2. Install the packages # Install devtools install.packages(&quot;devtools&quot;) # Install roxygen install.packages(&quot;roxygen2&quot;) 8.3 Building the most basic R package 8.4 Making A New R Project 8.5 Adding Documentation 8.6 Uploading and Installing from Github 8.7 Uploading and instaling it from CRAN 8.8 More Info and Additonational Resources "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
